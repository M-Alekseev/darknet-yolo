# Linux custom object detection using Darknet Yolo

## Prerequisites and Installation:

Before we start you need to install the following packages: [OpenCV](https://opencv.org/), [CUDA](https://developer.nvidia.com/cuda-downloads) (if you want GPU computation) and [Darknet](https://pjreddie.com/darknet/install/).

### Preparing a dataset
For this project I used [MIT Traffic Data Set](http://www.ee.cuhk.edu.hk/~xgwang/MITtraffic.html). They provide a set of videos recorded by stationary camera and 2 matlab files with manually labelled pedestrians that are available by the [link](http://www.ee.cuhk.edu.hk/~xgwang/MIT_traffic_ground_truth_data.tar.gz).

For further work we need to extract frames from videos, for example, by using [ffmpeg](https://www.ffmpeg.org/). If you're using Arch Linux you can install it with pacman
```
pacman -S ffmpeg
```
Then go to the folder with downloaded videos and extract frames of them. Use the next command:
```
ffmpeg -i mv2_001.avi mv2_001_$04d.png
```
where:
- -i means the input file
- mv2_001_$04d.png - the output frame name, $04d means that a numeration of the frame starts from 0001 and so on

Name the output files in that way because frame names are related with ground truth data that we've got earlier. Repeat this command for each video you have, changing names of input and output files.

I got almost 166k frames but we have labeled only each 200th frame. That's why we needed to name our frames as I said above. I used a python script to select labeled files and move them to a separate folder. Here is the [script](process_data.py).

Before starting the script make sure you have created folders for the train and test data, while execution it will ask you to select a groud truth data file, a folder with images and a folder to save.

If you're too lazy to do it, I prepared the ready to use [dataset](https://drive.google.com/file/d/0B-2U0T71FkkZNmpUdUNXRXlxUVE/view?usp=sharing).

After we finish with the dataset preparation, we should put it into darknet/data folder.

### Creating configuration files
We have to create it so darknet will know what files to train. We're going to create the following files:
- ground_truth.data
- train.list
- test.list
- names.list

So, let's start with ground_truth.data
```
classes = 1
train = data/pedestrian_ground_truth/train.list
valid = data/pedestrian_ground_truth/test.list
names = data/pedestrian_ground_truth/names.list
# you need to create darknet/backup dir to store your trained weights
backup = backup 
```
We're going to try train our network to find only persons, so we have only one class. train.list contains of paths to images that we'll use to train hence test.list contains of paths to test images.

Let's create the train.list, the easiest way to do it is to use a command (make sure that you're in folder with train and test folders) `find /test -name \*.png > test.list`

As a result you'll get something similar with this ![](https://image.ibb.co/gOM0BR/125.png)

Repeat this step for test images.

Create the names.list. One category on one string since we have only one it contains one string.
```
Person
```
Last step before we start to train our network is to create the .cfg file. To make it simplier I copied yolo-voc.cfg and made the following changes:
- Comment line 3 and 4 and uncomment line 5 and 6, because we're going to use it for train, when you'll be testing it change it back. So `batch = 64` means that we will use 64 images on each training step. `subdivisions = 8` means that the batch will be divided by 8 to decrease VRAM requirements. If you have a powerful GPU you can decrease it.
- line 8 and 9: `height = 608` and `width = 608` these parameters are responsible for network-resolution. You can use any value multiple of 32.
- line 237: You can find out amount of filters by `filters = (classes + 5) * 5`, so `filters = 30`
- line 244: Change the number `classes = 1`

To start training we also need a set of convulutional weights. We can get it from the [official site](https://pjreddie.com/media/files/darknet19_448.conv.23).

## Training

Time to start training. .data and .cfg files we put into /cfg. To start type into terminal the following command:
```
./darknet detector train cfg/pedestrian_ground_truth.data cfg/pedestrian.cfg darknet19_448.conv.23
```
You'll see the similar situation in your terminal ![](https://image.ibb.co/drPLBR/126.png)

If you have a *CUDA out of memory* error you may try to increase the number of `subdivisions` or change the `batch` size or change the network-resolution.

After each iteration we have a summary:

4: 314.345232, **332.818970 avg**, 0.001000 rate, 5.993169 seconds, 240 images

avg - means average loss, we should stop training at the lowest value possible. When avg starts to grow up it means that we have an overfitting and should stop further training.

Each 100th iteration darknet saves pretrained weights to the backup folder.

## Results

I stopped training after 700th iteration because `average loss` started to grow up and on tests cnn couldn't find any objects. I've tested weights that I've had and realized that 500th works the best in my situation. CNN is able to recognize pedestrians on images of this dataset without mistakes and can recognize on a video with some inaccuracies. So, to see results we should have pretrained `.weights` files. I recommend to use 500th weights because it gives the best results. To test type the following command (make sure that you've changed `batch size` and `subdivisions` to 1 in a config file):
```
./darknet detector test cfg/pedestrian_ground_truth.data cfg/pedestrian.cfg backup/pedestrian_500.weights data/pedestrian_ground_truth/test/test_image.png
```
![](https://image.ibb.co/cewOLm/127.png)

As a result we'll see an image (if OpenCV is enabed)

![](https://image.ibb.co/fOyEbR/128.png)

Of course sometimes when pedestrians just stuck in each other detector can not recognize them as 2 persons but at least we have found them. 

I've tested detector on images that weren't included to train/test data and it has shown quite good results. With our weights, detector shows much better accuracy than default `yolo.weights`.

Since detector might process video only with 3 frames/sec I decided to record it and then manually increase to make it looks normally. You can see results on the video below.

[![](https://image.ibb.co/kHc2Y6/123.png)](https://youtu.be/1w7PjwaMtwk)


